{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Yolo Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import services.ai_services.my_models as Models\n",
    "import services.image_handler.utils as IMUtils\n",
    "import services.ai_services.inferences as INF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yolo = Models.YOLOv8(Models.YoloType.Custom.Plate_best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = IMUtils.read_image('C:/Users/Alireza/Desktop/plate/2171038_FRONT.jpg')\n",
    "image_rgb = IMUtils.convert_to_rgb(image)\n",
    "image2 = IMUtils.read_image('C:/Users/Alireza/Desktop/plate/2170593_FRONT.jpg')\n",
    "image_rgb2 = IMUtils.convert_to_rgb(image2)\n",
    "IMUtils.show_image(image_rgb)\n",
    "IMUtils.show_image(image_rgb2)\n",
    "images = [image, image2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference = INF.YOLOInference(yolo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cropped_out = inference.run_full_pipeline(images)\n",
    "for i in cropped_out:\n",
    "    IMUtils.show_image(i, turn_grey=True, cmap='grey')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get and Save Plates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import services.image_handler.utils as IMUtils\n",
    "import services.ai_services.inferences as INF\n",
    "import services.ai_services.my_models as Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading images: 100%|██████████| 139/139 [00:02<00:00, 64.51file/s]\n"
     ]
    }
   ],
   "source": [
    "images = IMUtils.read_images_from_file(\"C:/Users/Alireza/Desktop/cars/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading YOLO model from weights/best(plate).pt ...\n",
      "\n",
      " =====================================================================================\n",
      "Layer (type:depth-idx)                                       Param #\n",
      "=====================================================================================\n",
      "YOLO                                                         --\n",
      "├─DetectionModel: 1-1                                        --\n",
      "│    └─Sequential: 2-1                                       --\n",
      "│    │    └─Conv: 3-1                                        (1,392)\n",
      "│    │    └─Conv: 3-2                                        (41,664)\n",
      "│    │    └─C2f: 3-3                                         (111,360)\n",
      "│    │    └─Conv: 3-4                                        (166,272)\n",
      "│    │    └─C2f: 3-5                                         (813,312)\n",
      "│    │    └─Conv: 3-6                                        (664,320)\n",
      "│    │    └─C2f: 3-7                                         (3,248,640)\n",
      "│    │    └─Conv: 3-8                                        (1,991,808)\n",
      "│    │    └─C2f: 3-9                                         (3,985,920)\n",
      "│    │    └─SPPF: 3-10                                       (831,168)\n",
      "│    │    └─Upsample: 3-11                                   --\n",
      "│    │    └─Concat: 3-12                                     --\n",
      "│    │    └─C2f: 3-13                                        (1,993,728)\n",
      "│    │    └─Upsample: 3-14                                   --\n",
      "│    │    └─Concat: 3-15                                     --\n",
      "│    │    └─C2f: 3-16                                        (517,632)\n",
      "│    │    └─Conv: 3-17                                       (332,160)\n",
      "│    │    └─Concat: 3-18                                     --\n",
      "│    │    └─C2f: 3-19                                        (1,846,272)\n",
      "│    │    └─Conv: 3-20                                       (1,327,872)\n",
      "│    │    └─Concat: 3-21                                     --\n",
      "│    │    └─C2f: 3-22                                        (4,207,104)\n",
      "│    │    └─Detect: 3-23                                     (1,141,075)\n",
      "=====================================================================================\n",
      "Total params: 23,221,699\n",
      "Trainable params: 0\n",
      "Non-trainable params: 23,221,699\n",
      "=====================================================================================\n"
     ]
    }
   ],
   "source": [
    "model = Models.YOLOv8(Models.YoloType.Custom.Plate_best)\n",
    "inference = INF.YOLOInference(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running full pipeline... \n",
      "\n",
      "conf=0.5, show=False, save=False\n",
      "Running inference on Image\n",
      "\n",
      "0: 640x640 1 plate, 19.2ms\n",
      "1: 640x640 1 plate, 19.2ms\n",
      "2: 640x640 1 plate, 19.2ms\n",
      "3: 640x640 1 plate, 19.2ms\n",
      "4: 640x640 2 plates, 19.2ms\n",
      "5: 640x640 8 plates, 19.2ms\n",
      "6: 640x640 6 plates, 19.2ms\n",
      "7: 640x640 1 plate, 19.2ms\n",
      "8: 640x640 1 plate, 19.2ms\n",
      "9: 640x640 1 plate, 19.2ms\n",
      "10: 640x640 2 plates, 19.2ms\n",
      "11: 640x640 1 plate, 19.2ms\n",
      "12: 640x640 1 plate, 19.2ms\n",
      "13: 640x640 1 plate, 19.2ms\n",
      "14: 640x640 1 plate, 19.2ms\n",
      "15: 640x640 1 plate, 19.2ms\n",
      "Speed: 17.1ms preprocess, 19.2ms inference, 6.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Yolo results are generated for batch 0 \n",
      "\n",
      "Found Bounding Boxes\n",
      "Images were cropped \n",
      "\n",
      "conf=0.5, show=False, save=False\n",
      "Running inference on Image\n",
      "\n",
      "0: 640x640 1 plate, 17.3ms\n",
      "1: 640x640 1 plate, 17.3ms\n",
      "2: 640x640 1 plate, 17.3ms\n",
      "3: 640x640 1 plate, 17.3ms\n",
      "4: 640x640 1 plate, 17.3ms\n",
      "5: 640x640 2 plates, 17.3ms\n",
      "6: 640x640 1 plate, 17.3ms\n",
      "7: 640x640 1 plate, 17.3ms\n",
      "8: 640x640 1 plate, 17.3ms\n",
      "9: 640x640 1 plate, 17.3ms\n",
      "10: 640x640 1 plate, 17.3ms\n",
      "11: 640x640 1 plate, 17.3ms\n",
      "12: 640x640 1 plate, 17.3ms\n",
      "13: 640x640 1 plate, 17.3ms\n",
      "14: 640x640 1 plate, 17.3ms\n",
      "15: 640x640 1 plate, 17.3ms\n",
      "Speed: 11.1ms preprocess, 17.3ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Yolo results are generated for batch 1 \n",
      "\n",
      "Found Bounding Boxes\n",
      "Images were cropped \n",
      "\n",
      "conf=0.5, show=False, save=False\n",
      "Running inference on Image\n",
      "\n",
      "0: 640x640 1 plate, 17.3ms\n",
      "1: 640x640 1 plate, 17.3ms\n",
      "2: 640x640 1 plate, 17.3ms\n",
      "3: 640x640 2 plates, 17.3ms\n",
      "4: 640x640 1 plate, 17.3ms\n",
      "5: 640x640 1 plate, 17.3ms\n",
      "6: 640x640 2 plates, 17.3ms\n",
      "7: 640x640 1 plate, 17.3ms\n",
      "8: 640x640 1 plate, 17.3ms\n",
      "9: 640x640 1 plate, 17.3ms\n",
      "10: 640x640 1 plate, 17.3ms\n",
      "11: 640x640 2 plates, 17.3ms\n",
      "12: 640x640 2 plates, 17.3ms\n",
      "13: 640x640 1 plate, 17.3ms\n",
      "14: 640x640 2 plates, 17.3ms\n",
      "15: 640x640 1 plate, 17.3ms\n",
      "Speed: 7.1ms preprocess, 17.3ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Yolo results are generated for batch 2 \n",
      "\n",
      "Found Bounding Boxes\n",
      "Images were cropped \n",
      "\n",
      "conf=0.5, show=False, save=False\n",
      "Running inference on Image\n",
      "\n",
      "0: 640x640 1 plate, 17.3ms\n",
      "1: 640x640 2 plates, 17.3ms\n",
      "2: 640x640 1 plate, 17.3ms\n",
      "3: 640x640 2 plates, 17.3ms\n",
      "4: 640x640 1 plate, 17.3ms\n",
      "5: 640x640 2 plates, 17.3ms\n",
      "6: 640x640 1 plate, 17.3ms\n",
      "7: 640x640 1 plate, 17.3ms\n",
      "8: 640x640 2 plates, 17.3ms\n",
      "9: 640x640 1 plate, 17.3ms\n",
      "10: 640x640 1 plate, 17.3ms\n",
      "11: 640x640 1 plate, 17.3ms\n",
      "12: 640x640 1 plate, 17.3ms\n",
      "13: 640x640 1 plate, 17.3ms\n",
      "14: 640x640 2 plates, 17.3ms\n",
      "15: 640x640 2 plates, 17.3ms\n",
      "Speed: 9.3ms preprocess, 17.3ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Yolo results are generated for batch 3 \n",
      "\n",
      "Found Bounding Boxes\n",
      "Images were cropped \n",
      "\n",
      "conf=0.5, show=False, save=False\n",
      "Running inference on Image\n",
      "\n",
      "0: 640x640 2 plates, 17.2ms\n",
      "1: 640x640 1 plate, 17.2ms\n",
      "2: 640x640 1 plate, 17.2ms\n",
      "3: 640x640 (no detections), 17.2ms\n",
      "4: 640x640 1 plate, 17.2ms\n",
      "5: 640x640 1 plate, 17.2ms\n",
      "6: 640x640 2 plates, 17.2ms\n",
      "7: 640x640 1 plate, 17.2ms\n",
      "8: 640x640 1 plate, 17.2ms\n",
      "9: 640x640 1 plate, 17.2ms\n",
      "10: 640x640 1 plate, 17.2ms\n",
      "11: 640x640 2 plates, 17.2ms\n",
      "12: 640x640 2 plates, 17.2ms\n",
      "13: 640x640 (no detections), 17.2ms\n",
      "14: 640x640 1 plate, 17.2ms\n",
      "15: 640x640 1 plate, 17.2ms\n",
      "Speed: 13.8ms preprocess, 17.2ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Yolo results are generated for batch 4 \n",
      "\n",
      "Found Bounding Boxes\n",
      "Images were cropped \n",
      "\n",
      "conf=0.5, show=False, save=False\n",
      "Running inference on Image\n",
      "\n",
      "0: 640x640 1 plate, 17.2ms\n",
      "1: 640x640 1 plate, 17.2ms\n",
      "2: 640x640 1 plate, 17.2ms\n",
      "3: 640x640 1 plate, 17.2ms\n",
      "4: 640x640 3 plates, 17.2ms\n",
      "5: 640x640 1 plate, 17.2ms\n",
      "6: 640x640 1 plate, 17.2ms\n",
      "7: 640x640 1 plate, 17.2ms\n",
      "8: 640x640 3 plates, 17.2ms\n",
      "9: 640x640 3 plates, 17.2ms\n",
      "10: 640x640 2 plates, 17.2ms\n",
      "11: 640x640 1 plate, 17.2ms\n",
      "12: 640x640 1 plate, 17.2ms\n",
      "13: 640x640 1 plate, 17.2ms\n",
      "14: 640x640 1 plate, 17.2ms\n",
      "15: 640x640 1 plate, 17.2ms\n",
      "Speed: 9.7ms preprocess, 17.2ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Yolo results are generated for batch 5 \n",
      "\n",
      "Found Bounding Boxes\n",
      "Images were cropped \n",
      "\n",
      "conf=0.5, show=False, save=False\n",
      "Running inference on Image\n",
      "\n",
      "0: 640x640 2 plates, 17.3ms\n",
      "1: 640x640 1 plate, 17.3ms\n",
      "2: 640x640 2 plates, 17.3ms\n",
      "3: 640x640 1 plate, 17.3ms\n",
      "4: 640x640 2 plates, 17.3ms\n",
      "5: 640x640 2 plates, 17.3ms\n",
      "6: 640x640 2 plates, 17.3ms\n",
      "7: 640x640 2 plates, 17.3ms\n",
      "8: 640x640 3 plates, 17.3ms\n",
      "9: 640x640 1 plate, 17.3ms\n",
      "10: 640x640 1 plate, 17.3ms\n",
      "11: 640x640 2 plates, 17.3ms\n",
      "12: 640x640 1 plate, 17.3ms\n",
      "13: 640x640 1 plate, 17.3ms\n",
      "14: 640x640 1 plate, 17.3ms\n",
      "15: 640x640 1 plate, 17.3ms\n",
      "Speed: 12.9ms preprocess, 17.3ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Yolo results are generated for batch 6 \n",
      "\n",
      "Found Bounding Boxes\n",
      "Images were cropped \n",
      "\n",
      "conf=0.5, show=False, save=False\n",
      "Running inference on Image\n",
      "\n",
      "0: 640x640 1 plate, 17.2ms\n",
      "1: 640x640 2 plates, 17.2ms\n",
      "2: 640x640 1 plate, 17.2ms\n",
      "3: 640x640 1 plate, 17.2ms\n",
      "4: 640x640 1 plate, 17.2ms\n",
      "5: 640x640 1 plate, 17.2ms\n",
      "6: 640x640 1 plate, 17.2ms\n",
      "7: 640x640 (no detections), 17.2ms\n",
      "8: 640x640 2 plates, 17.2ms\n",
      "9: 640x640 1 plate, 17.2ms\n",
      "10: 640x640 3 plates, 17.2ms\n",
      "11: 640x640 1 plate, 17.2ms\n",
      "12: 640x640 1 plate, 17.2ms\n",
      "13: 640x640 1 plate, 17.2ms\n",
      "14: 640x640 1 plate, 17.2ms\n",
      "15: 640x640 1 plate, 17.2ms\n",
      "Speed: 12.7ms preprocess, 17.2ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Yolo results are generated for batch 7 \n",
      "\n",
      "Found Bounding Boxes\n",
      "Images were cropped \n",
      "\n",
      "conf=0.5, show=False, save=False\n",
      "Running inference on Image\n",
      "\n",
      "0: 640x640 2 plates, 20.2ms\n",
      "1: 640x640 1 plate, 20.2ms\n",
      "2: 640x640 2 plates, 20.2ms\n",
      "3: 640x640 1 plate, 20.2ms\n",
      "4: 640x640 1 plate, 20.2ms\n",
      "5: 640x640 1 plate, 20.2ms\n",
      "6: 640x640 1 plate, 20.2ms\n",
      "7: 640x640 1 plate, 20.2ms\n",
      "8: 640x640 4 plates, 20.2ms\n",
      "9: 640x640 (no detections), 20.2ms\n",
      "10: 640x640 1 plate, 20.2ms\n",
      "Speed: 4.4ms preprocess, 20.2ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Yolo results are generated for batch 8 \n",
      "\n",
      "Found Bounding Boxes\n",
      "Images were cropped \n",
      "\n",
      "All images were cropped successfully\n"
     ]
    }
   ],
   "source": [
    "cropped_out = inference.run_full_pipeline(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving images: 100%|██████████| 190/190 [00:00<00:00, 689.37file/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files were saved successfully in C:/Users/Alireza/Desktop/plates/ \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "output_dir = \"C:/Users/Alireza/Desktop/plates/\"\n",
    "IMUtils.save_cropped_images(cropped_out, output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.02825701, 0.09577547]),\n",
       " array([0.03425093, 0.86719203]),\n",
       " array([0.97101385, 0.8219808 ]),\n",
       " array([0.96844503, 0.07316985]))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dst_points = np.array([[0, 0], [image_width, 0], [image_width, image_height], [0, image_height]], dtype=np.float32)\n",
    "src_points  = np.array(list(shape_corners.values()), dtype=np.float32)\n",
    "\n",
    "M = cv2.getPerspectiveTransform(src_points, dst_points)\n",
    "rectified_image = cv2.warpPerspective(gray_image_1, M, (image_width, image_height))\n",
    "\n",
    "plt.imshow(rectified_image, cmap='gray')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from services.ai_services.my_models import Plate_ResNet\n",
    "import services.ai_services.utils as AIUtils\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NVIDIA GeForce GTX 1660 Ti have been located and selected\n",
      "Selected model is ResNetType.Base\n",
      "ResNet18\n",
      "======================================================================\n",
      "Layer (type:depth-idx)                        Param #\n",
      "======================================================================\n",
      "CustomResNet                                  --\n",
      "├─ResNet: 1-1                                 --\n",
      "│    └─Conv2d: 2-1                            9,408\n",
      "│    └─BatchNorm2d: 2-2                       128\n",
      "│    └─ReLU: 2-3                              --\n",
      "│    └─MaxPool2d: 2-4                         --\n",
      "│    └─Sequential: 2-5                        --\n",
      "│    │    └─BasicBlock: 3-1                   73,984\n",
      "│    │    └─BasicBlock: 3-2                   73,984\n",
      "│    └─Sequential: 2-6                        --\n",
      "│    │    └─BasicBlock: 3-3                   230,144\n",
      "│    │    └─BasicBlock: 3-4                   295,424\n",
      "│    └─Sequential: 2-7                        --\n",
      "│    │    └─BasicBlock: 3-5                   919,040\n",
      "│    │    └─BasicBlock: 3-6                   1,180,672\n",
      "│    └─Sequential: 2-8                        --\n",
      "│    │    └─BasicBlock: 3-7                   3,673,088\n",
      "│    │    └─BasicBlock: 3-8                   4,720,640\n",
      "│    └─AdaptiveAvgPool2d: 2-9                 --\n",
      "│    └─Dropout: 2-10                          --\n",
      "├─Sequential: 1-2                             --\n",
      "│    └─Linear: 2-11                           65,664\n",
      "│    └─SiLU: 2-12                             --\n",
      "│    └─Linear: 2-13                           4,128\n",
      "│    └─SiLU: 2-14                             --\n",
      "├─Linear: 1-3                                 66\n",
      "├─Linear: 1-4                                 66\n",
      "├─Linear: 1-5                                 66\n",
      "├─Linear: 1-6                                 66\n",
      "======================================================================\n",
      "Total params: 11,246,568\n",
      "Trainable params: 11,246,568\n",
      "Non-trainable params: 0\n",
      "====================================================================== \n",
      "\n"
     ]
    }
   ],
   "source": [
    "device = AIUtils.select_device()\n",
    "res = Plate_ResNet(device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start freezing feature extractor ...\n",
      "Feature extractor freezed\n",
      "Model is in cuda\n",
      "======================================================================\n",
      "Layer (type:depth-idx)                        Param #\n",
      "======================================================================\n",
      "CustomResNet                                  --\n",
      "├─ResNet: 1-1                                 --\n",
      "│    └─Conv2d: 2-1                            (9,408)\n",
      "│    └─BatchNorm2d: 2-2                       (128)\n",
      "│    └─ReLU: 2-3                              --\n",
      "│    └─MaxPool2d: 2-4                         --\n",
      "│    └─Sequential: 2-5                        --\n",
      "│    │    └─BasicBlock: 3-1                   (73,984)\n",
      "│    │    └─BasicBlock: 3-2                   (73,984)\n",
      "│    └─Sequential: 2-6                        --\n",
      "│    │    └─BasicBlock: 3-3                   (230,144)\n",
      "│    │    └─BasicBlock: 3-4                   (295,424)\n",
      "│    └─Sequential: 2-7                        --\n",
      "│    │    └─BasicBlock: 3-5                   (919,040)\n",
      "│    │    └─BasicBlock: 3-6                   (1,180,672)\n",
      "│    └─Sequential: 2-8                        --\n",
      "│    │    └─BasicBlock: 3-7                   (3,673,088)\n",
      "│    │    └─BasicBlock: 3-8                   (4,720,640)\n",
      "│    └─AdaptiveAvgPool2d: 2-9                 --\n",
      "│    └─Dropout: 2-10                          --\n",
      "├─Sequential: 1-2                             --\n",
      "│    └─Linear: 2-11                           65,664\n",
      "│    └─SiLU: 2-12                             --\n",
      "│    └─Linear: 2-13                           4,128\n",
      "│    └─SiLU: 2-14                             --\n",
      "├─Linear: 1-3                                 66\n",
      "├─Linear: 1-4                                 66\n",
      "├─Linear: 1-5                                 66\n",
      "├─Linear: 1-6                                 66\n",
      "======================================================================\n",
      "Total params: 11,246,568\n",
      "Trainable params: 70,056\n",
      "Non-trainable params: 11,176,512\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "res.freeze_unfreeze(True)\n",
    "res.write_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = AIUtils.CustomResNetDataset(\"C:/Users/Alireza/Desktop/plate_data/train/images\",\n",
    "                                      \"C:/Users/Alireza/Desktop/plate_data/train/labels\")\n",
    "loader = DataLoader(dataset, 16, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = loader.__iter__().__next__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 4, 2])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running inference on Image Batch\n",
      "Outputs Generated\n"
     ]
    }
   ],
   "source": [
    "a = res.infer(torch.rand([16,3,224,224]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 4, 2])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
